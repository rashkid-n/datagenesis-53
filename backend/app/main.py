
from fastapi import FastAPI, HTTPException, Depends, WebSocket, WebSocketDisconnect, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import logging
import sys
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List
import uuid

# Configure logging properly
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="DataGenesis AI API",
    description="Enterprise-grade synthetic data generation platform",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# Configure CORS - CRITICAL: Allow all origins for development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for development
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import services and routes
from .routes import generation, agents, analytics, auth, datasets, ai_config
from .services.gemini_service import GeminiService
from .services.ollama_service import OllamaService
from .services.agent_orchestrator import AgentOrchestrator
from .services.websocket_manager import ConnectionManager
from .services.ai_service import ai_service

# Initialize services
gemini_service = GeminiService()
ollama_service = OllamaService()
orchestrator = AgentOrchestrator(ollama_service=ollama_service)
websocket_manager = ConnectionManager()

# Optional authentication
class OptionalHTTPBearer(HTTPBearer):
    async def __call__(self, request: Request):
        try:
            return await super().__call__(request)
        except HTTPException:
            return None

security = OptionalHTTPBearer(auto_error=False)

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    logger.info("üöÄ Starting DataGenesis AI API...")
    await gemini_service.initialize()
    await ollama_service.initialize()
    await orchestrator.initialize()
    
    # Log initialization status without consuming quota
    gemini_status = "initialized" if gemini_service.is_initialized else "not configured"
    ollama_status = "initialized" if ollama_service.is_initialized else "not available"
    logger.info(f"ü§ñ Gemini Service: {gemini_status}")
    logger.info(f"ü¶ô Ollama Service: {ollama_status}")
    
    logger.info("üéØ DataGenesis AI API started successfully!")

@app.on_event("shutdown") 
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("üì¥ Shutting down DataGenesis AI API...")
    logger.info("üì¥ DataGenesis AI API shutdown complete")

@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = datetime.utcnow()
    
    response = await call_next(request)
    
    process_time = (datetime.utcnow() - start_time).total_seconds()
    if process_time > 1.0:  # Only log slow requests
        logger.info(f"{request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s")
    
    return response

# Include route modules
app.include_router(generation.router)
app.include_router(agents.router)
app.include_router(analytics.router)
app.include_router(auth.router)
app.include_router(datasets.router)
app.include_router(ai_config.router)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DataGenesis AI Backend",
        "version": "1.0.0",
        "status": "running",
        "api_docs": "/api/docs",
        "health": "/api/health"
    }

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    # Check AI services health
    gemini_status = await gemini_service.health_check()
    ollama_status = await ollama_service.health_check()
    
    # Determine overall AI availability
    ai_available = gemini_status.get("status") == "online" or ollama_status.get("status") == "online"
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "environment": "production",
        "message": "DataGenesis AI is running",
        "ai_available": ai_available,
        "services": {
            "gemini": gemini_status,
            "ollama": ollama_status,
            "agents": "active",
            "websockets": "ready"
        }
    }
    
    return health_status

@app.options("/{full_path:path}")
async def options_handler(full_path: str):
    """Handle preflight OPTIONS requests"""
    return JSONResponse(
        content={"message": "OK"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """WebSocket endpoint for real-time updates - Fixed authentication"""
    try:
        # Allow connections without authentication for guest users
        await websocket_manager.connect(websocket, client_id)
        logger.info(f"üîå WebSocket connected: {client_id}")
        
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # Handle different message types
            if message.get("type") == "ping":
                await websocket_manager.send_personal_message(
                    json.dumps({"type": "pong", "timestamp": datetime.utcnow().isoformat()}),
                    client_id
                )
            
    except WebSocketDisconnect:
        websocket_manager.disconnect(client_id)
        logger.info(f"üîå WebSocket disconnected: {client_id}")
    except Exception as e:
        logger.error(f"‚ùå WebSocket error for {client_id}: {str(e)}")
        websocket_manager.disconnect(client_id)

@app.post("/api/generation/schema-from-description")
async def generate_schema_from_description(
    request: dict,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """Generate schema from natural language description using configured AI service"""
    description = request.get("description", "")
    domain = request.get("domain", "general")
    data_type = request.get("data_type", "tabular")
    
    # Basic validation
    if not description or len(description.strip()) < 10:
        raise HTTPException(status_code=400, detail="Description must be at least 10 characters long")
    
    try:
        # Try using configured AI service first, fallback to Gemini
        if ai_service.is_initialized:
            schema_result = await ai_service.generate_schema_from_natural_language(
                description, domain, data_type
            )
        else:
            schema_result = await gemini_service.generate_schema_from_natural_language(
                description, domain, data_type
            )
        
        return {
            "schema": schema_result.get('schema', {}),
            "detected_domain": schema_result.get('detected_domain', domain),
            "estimated_rows": schema_result.get('estimated_rows', 10000),
            "suggestions": schema_result.get('suggestions', []),
            "sample_data": schema_result.get('sample_data', [])
        }
        
    except Exception as e:
        logger.error(f"Schema generation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Schema generation failed: {str(e)}")

@app.post("/api/generation/generate-local")
async def generate_synthetic_data(
    request: dict,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """Generate enterprise-grade synthetic data using configured AI service or orchestrator"""
    schema = request.get('schema', {})
    config = request.get('config', {})
    description = request.get('description', '')
    source_data = request.get('sourceData', [])
    
    try:
        # If AI service is configured, use it directly for faster generation
        if ai_service.is_initialized:
            logger.info(f"üéØ Using configured AI service: {ai_service.current_provider}")
            result = await ai_service.generate_synthetic_data_advanced(
                schema=schema,
                config=config,
                description=description
            )
            
            return {
                "synthetic_data": result,
                "message": f"Generated using {ai_service.current_provider}",
                "records_generated": len(result),
                "provider": ai_service.current_provider,
                "model": ai_service.current_model
            }
        else:
            # Fallback to orchestrator with Ollama
            logger.info("üîÑ Using agent orchestrator as fallback")
            job_id = str(uuid.uuid4())
            result = await orchestrator.orchestrate_generation(
                job_id=job_id,
                source_data=source_data,
                schema=schema,
                config=config,
                description=description,
                websocket_manager=websocket_manager
            )
            return result
        
    except Exception as e:
        logger.error(f"Generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/api/generation/analyze")
async def analyze_data(
    request: dict,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """Analyze uploaded data using AI"""
    sample_data = request.get('sample_data', [])
    config = request.get('config', {})
    
    if not sample_data:
        raise HTTPException(status_code=400, detail="No sample data provided")
    
    try:
        analysis = await gemini_service.analyze_schema_advanced(sample_data, config, [])
        
        return {
            "analysis": analysis,
            "recommendations": analysis.get('recommendations', {})
        }
        
    except Exception as e:
        logger.error(f"Data analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/api/agents/status")
async def get_agents_status():
    """Get real-time status of all AI agents"""
    logger.info("üìä Agent status requested")
    
    status = await orchestrator.get_agents_status()
    
    logger.info("‚úÖ Agent status retrieved")
    return status

@app.get("/api/system/status")
async def system_status(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Get real-time system status"""
    logger.info("üìä System status requested")
    
    # Get comprehensive system status
    gemini_status = await gemini_service.health_check()
    agents_status = await orchestrator.get_agents_status()
    
    status = {
        "timestamp": datetime.utcnow().isoformat(),
        "services": {
            "gemini_2_flash": gemini_status,
            "multi_agent_system": agents_status,
            "websockets": "active",
            "real_time_logging": "enabled"
        },
        "performance_metrics": {
            "ai_processing": "optimal",
            "response_time": "< 100ms",
            "uptime": "99.9%"
        }
    }
    
    logger.info("‚úÖ System status compiled")
    return status

@app.post("/api/gemini/test-connection")
async def test_gemini_connection(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Test actual Gemini API connection - only call when needed"""
    logger.info("üß™ Testing Gemini API connection requested")
    
    try:
        result = await gemini_service.test_api_connection()
        logger.info(f"üß™ API test result: {result}")
        return result
    except Exception as e:
        logger.error(f"‚ùå API test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"API test failed: {str(e)}")

@app.post("/api/gemini/switch-model")
async def switch_gemini_model(
    request: dict,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """Switch to a different Gemini model to avoid quota issues"""
    logger.info("üîÑ Gemini model switch requested")
    
    model_name = request.get("model", "gemini-1.5-flash")
    
    try:
        # Switch to a different model that might have quota available
        result = await gemini_service.switch_model(model_name)
        logger.info(f"üîÑ Model switch result: {result}")
        return result
    except Exception as e:
        logger.error(f"‚ùå Model switch failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Model switch failed: {str(e)}")

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"‚ùå Unhandled exception in {request.method} {request.url.path}: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"error": "Internal server error", "detail": str(exc)}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
